{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Maluuba_Microsoft_Brandmark_Colour.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Language\n",
    "\n",
    "## A tutorial prepared by Maluuba\n",
    "\n",
    "Prepared by Justin Harris, Tavian Barnes, and Adam Atkinson for the [ImplementAI Hackathon](https://implementai.com/) run by the [McGill A.I. Society](https://mcgillai.com/)\n",
    "\n",
    "## About this tutorial\n",
    "\n",
    "This notebook will teach the audience how deep learning is used for natural language processing (NLP) tasks, including:\n",
    "\n",
    "- Named Entity Recognition\n",
    "- Part of Speech Tagging & Syntactic Parsing\n",
    "- Language Modelling\n",
    "- Natural Language Generation\n",
    "- Intent Classification\n",
    "- Translation\n",
    "- Question-Answering\n",
    "- Dialogue\n",
    "\n",
    "We introduce the following concepts necessary to build a \"deep NLP\" system:\n",
    "\n",
    "- Word embeddings\n",
    "- Recurrent neural networks\n",
    "- Deep architectures for NLP\n",
    "\n",
    "We also motivate and demonstrate these ideas with a sample model, inspired by Maluuba's own work on a new task: generating questions from source texts. See our recent research here: [(1)](https://arxiv.org/abs/1706.01450), [(2)](https://arxiv.org/abs/1705.02012).\n",
    "\n",
    "For the purposes of this tutorial, some knowledge of machine learning, neural networks, and natural language processing will be helpful.\n",
    "\n",
    "## About [Maluuba](https://www.maluuba.com)\n",
    "\n",
    "Maluuba develops artificial intelligence that understands language. Our mission is to build a literate machine.\n",
    "\n",
    "We research problems and develop solutions related to:\n",
    "\n",
    "- Machine reading comprehension\n",
    "- Dialogue systems\n",
    "- Reinforcement learning\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. <a href=\"#pipeline\">Deep NLP Pipeline</a>\n",
    "1. <a href=\"#word-embeddings\">Word Embeddings</a>\n",
    "1. <a href=\"#rnns\">Recurrent Neural Networks</a>\n",
    "1. <a href=\"#qgen\">Question Generation</a>\n",
    "1. <a href=\"#advanced-rnns\">Architectures and Advanced RNNs</a>\n",
    "1. <a href=\"#references\">References</a>\n",
    "1. <a href=\"#resources\">Resources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline\"></a>\n",
    "# Deep NLP Pipeline\n",
    "\n",
    "This pipeline is for **supervised** learning approaches (i.e. all training data has labels).\n",
    "\n",
    "### 1. Pre-process the corpus\n",
    "\n",
    "This means cleaning your data and getting it into the format you want. You'll strip characters, fold case, **tokenize**, and maybe **lemmatize**.\n",
    "\n",
    "### 2. Prepare embeddings\n",
    "\n",
    "These can be computed yourself, learned as a part of your model, or you can use pre-trained ones (e.g. GoogleNews word2vec vectors, Stanford GloVe vectors from Wikipedia or Common Crawl).\n",
    "\n",
    "### 3. Define input and output representations\n",
    "\n",
    "You need to define:\n",
    "1. How input is **encoded** for your model. \n",
    "1. How output is **encoded** for your model.\n",
    "1. How output is **decoded** from your model into something that is meaningful to humans. This is done by looking at the class that maximizes the probability of a class or word, or the probability distribution itself.\n",
    "\n",
    "You'll need to consider things like sequence lengths, input/output vector or **tensor** representations, **padding**, and **out-of-vocabulary (OOV)** words.\n",
    "\n",
    "Make sure that the inputs and outputs are neural network friendly. This may involve a **one-hot vector** encoding, **standardizing** values, **scaling** values, or centering values around zero. \n",
    "\n",
    "You may use things like a **softmax layer** or **beam search** in the decoding step.\n",
    "\n",
    "### 4. Construct the model\n",
    "\n",
    "1. **Input**: the X values, features, or co-variates of your formatted data.\n",
    "1. **(Optional)** Embedding layer: if your input isn't word vectors but needs them, then this layer will transform your input using some word embeddings.\n",
    "1. **Your neural model**: any combination of differentiable units and modules.\n",
    "1. **Output**: the Y values, predictions, or targets corresponding to your formatted data.\n",
    "\n",
    "### 5. Train the model\n",
    "\n",
    "Use **backpropagation** with your favourite loss functions on your model, using labelled training data, to learn model parameters.\n",
    "\n",
    "### 6. Use the model\n",
    "\n",
    "Use your model to make predictions on unseen data. This will involve encoding your input and decoding the output to a friendly format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word-embeddings\"></a>\n",
    "# Word Embeddings\n",
    "\n",
    "**Problem**: How do we represent words numerically?\n",
    "\n",
    "- Can we use the index to a word in the vocabulary?\n",
    "- What if the vocabulary is large, how do we save space?\n",
    "- How do we decode this representation back into text?\n",
    "- Can we capture any semantic information or context in this representation?\n",
    "\n",
    "**Solution**: Word embeddings!\n",
    "\n",
    "- Use a neural network to predict a word given the words around it (**continuous bag of words/CBOW**), or predict the words around a given word (**skipgram**).\n",
    "- Use one-hot encodings of the words as input and output, keep the neural network smaller than the length of the vocabulary/one-hot vectors, then take the weights of the neural network as the word vectors!\n",
    "- Think of word vectors as columns in a matrix. Then the data is transformed by multiplying the one-hot vector by this matrix.\n",
    "- Train the neural network over a lot of text (a large **corpus**)\n",
    "- Word embeddings are compact, dense, and also embed sematic information.\n",
    "\n",
    "<img src=\"assets/word_embeddings.png\" width=\"60%\"/>\n",
    "(Source: https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rnns\"></a>\n",
    "# Recurrent Neural Networks\n",
    "\n",
    "Language is **sequential**, so we need models that can work with data in this way.\n",
    "\n",
    "**Recurrent neural networks (RNNs)** work across time slices of an input while maintaining some **state** over time.\n",
    "\n",
    "A recurrent module takes input at a timestep $t$ and uses the input and state at previous timestep $t-1$ to compute \n",
    "1. the output at timestep $t$.\n",
    "1. the updated state at timestep $t$.\n",
    "\n",
    "A recurrent module can be any type of stateful differentiable neural model.\n",
    "\n",
    "You can think of an RNN as a really narrow but deep feedforward/vanilla neural network or multilayer perceptron (MLP). We train an RNN by **unrolling** the computation in time and backpropagating over the sequence.\n",
    "\n",
    "### Problems\n",
    "\n",
    "RNNs have had these issues in the past:\n",
    "\n",
    "1) Long sequences => really deep neural network => computationally intense.\n",
    "- **Solution**: don't backpropagate through the entire sequence, use **truncated backpropagation through time**.\n",
    "    \n",
    "2) RNNs are bad at remembering things over longer time spans.\n",
    "- **Solution**: control the way state is updated or forgetten using units like **GRU** and **LSTM**.\n",
    "    \n",
    "3) Gradient signals between timesteps get smaller and become zero (**vanishing gradients**).\n",
    "- **Solution**: neural units that prevent this, like **GRU** and **LSTM**.\n",
    "    \n",
    "4) Gradient signals between timesteps grow exponentially (**exploding gradients**).\n",
    "- **Solution**: **clip** the gradients so they never get larger than a certain value.\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "- Long Short-Term Memory.\n",
    "- Has two forms of memory: \"hidden\" state and **cell state**\n",
    "    - Hidden state is also the output for the unit.\n",
    "    - Cell state represents how we want to update the hidden state.\n",
    "- Has **three** gates: \n",
    "    - **forget** gate updates the cell state by determining what part of the hidden state to forget.\n",
    "    - **input** gate updates the cell state by determining how input will affect the hidden state.\n",
    "    - **output** gate combines the cell and hidden states.\n",
    "- More parameters makes it more expressive but more computationally intensive.\n",
    "\n",
    "<img src=\"assets/LSTM.png\" width=\"35%\"/>\n",
    "(Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "### GRU\n",
    "\n",
    "- Gated Recurrent Unit.\n",
    "- GRU is a simplification of LSTM.\n",
    "- One form of memory: hidden state.\n",
    "- Has **two** gates which produce the output and new hidden state: \n",
    "    - **reset** gate combines input with the previous state.\n",
    "    - **update** gate discards part of the previous state.\n",
    "- Fewer parameters makes it less computationally intensive, but this unit has less capacity and is less expressive.\n",
    "\n",
    "<img src=\"assets/GRU.png\" width=\"40%\"/>\n",
    "(Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"#qgen\"></a>\n",
    "# Question Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced-rnns\"></a>\n",
    "# Architectures and Advanced RNNs\n",
    "\n",
    "### Input-Output Cardinality\n",
    "\n",
    "The task you choose is dependent on how many inputs and outputs there are and how they are distributed across time.\n",
    "\n",
    "**One-to-one**: Take the whole sequence at once and predict one value for the entire sequence.\n",
    "\n",
    "**One-to-many**: Take the whole sequence at once and predict multiple subsequent values.\n",
    "\n",
    "**Many-to-many**: Take the sequence one word at a time and predict something for each word. This is commonly known as **sequence-to-sequence (seq2seq)**.\n",
    "\n",
    "**Many-to-one**: Take the sequence one word at a time, predict something for each word, then combine the outputs for each time step into a single output. This is often achieved by **pooling** over the outputs at each timestep.\n",
    "\n",
    "<img src=\"assets/n_to_n_diagram.jpeg\" width=\"75%\"/>\n",
    "(Source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "### Encoder-Decoder\n",
    "\n",
    "- Many neural models for language use an intermediate representation for the data.\n",
    "- Two separate models called an **encoder** and a **decoder** write to and read from this format.\n",
    "- The encoder and decoder are stacked on top of each other, and the decoder reads from the intermediate representation after the encoder has finished writing to it.\n",
    "\n",
    "<img src=\"assets/encoder_decoder.jpg\" width=\"50%\"/>\n",
    "(Source: https://talbaumel.github.io/attention/)\n",
    "\n",
    "\n",
    "### Attention\n",
    "\n",
    "- Gives the neural network the ability to \"focus\" on part of the input.\n",
    "- An attentive decoder learns a weighted combination of all time-distributed features, or intermediate representation, produced by the layer below (i.e. the encoder). \n",
    "- The weighting parameters are learned and determine the strength of the features from the encoder at each time step.\n",
    "\n",
    "<img src=\"assets/attention.png\" width=\"75%\"/>\n",
    "(Source: https://distill.pub/2016/augmented-rnns/)\n",
    "\n",
    "### Pointer Networks\n",
    "\n",
    "- We interpret the token index that maximizes the attention at a time step to be a pointer.\n",
    "- We look at the attention for two consecutive time steps - the start and end of a segment - meaning we only unroll twice.\n",
    "\n",
    "<img src=\"assets/pointer_net.png\" width=\"75%\"/>\n",
    "(Source: https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Model\n",
    "\n",
    "Here is our model to generate questions from a document.  It uses many of the above concepts in order to demonstrate combining them into one model.  This model is a simplification of a few ideas from some of our recent papers on question generation.\n",
    "\n",
    "Let's start with detecting potentional answers in documents.  The following is a simple entity recognition model.\n",
    "\n",
    "<img src=\"assets/answer-tagging-model.png\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to train a word embedding from scratch, but it's usually better to start with a pre-trained word embedding.  We'll use the Stanford's GloVe embeddings.\n",
    "\n",
    "(https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from qgen.embedding import glove\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", initializer=glove)\n",
    "\n",
    "EMBEDDING_DIMENS = glove.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the part of the model that finds potential answers in the document.  We use a bidirectional recurrent network to predict, for each word in the document, whether it's part of an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import GRUCell\n",
    "\n",
    "document_tokens = tf.placeholder(tf.int32, shape=[None, None], name=\"document_tokens\")\n",
    "document_lengths = tf.placeholder(tf.int32, shape=[None], name=\"document_lengths\")\n",
    "\n",
    "document_emb = tf.nn.embedding_lookup(embedding, document_tokens)\n",
    "\n",
    "forward_cell = GRUCell(EMBEDDING_DIMENS)\n",
    "backward_cell = GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, document_lengths, dtype=tf.float32,\n",
    "    scope=\"answer_rnn\")\n",
    "answer_outputs = tf.concat(answer_outputs, 2)\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the model, we'll feed it the expected answers from the training set, and compute how far away the models predictions were from them.  The optimizer will try to minimize this value, known as the *loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "answer_labels = tf.placeholder(tf.int32, shape=[None, None], name=\"answer_labels\")\n",
    "\n",
    "answer_mask = tf.sequence_mask(document_lengths, dtype=tf.float32)\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=answer_labels, weights=answer_mask, name=\"answer_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate questions from the answers, we'll use a sequence-to-sequence model that transforms an answer into a question.  We can feed it the recurrent states from the answer-finding RNN above to give each answer some context from the rest of the document.\n",
    "\n",
    "<img src=\"assets/end-to-end-model.png\" width=\"90%\"/>\n",
    "\n",
    "First, the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_mask = tf.placeholder(\n",
    "    tf.float32, shape=[None, None, None], name=\"encoder_input_mask\")\n",
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "encoder_cell = GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "\n",
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the decoder.  It takes the last state of the encoder as input, and begins generating question words one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_inputs\")\n",
    "decoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"decoder_lengths\")\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs)\n",
    "helper = seq2seq.TrainingHelper(decoder_emb, decoder_lengths)\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False)\n",
    "\n",
    "decoder_cell = GRUCell(encoder_cell.state_size)\n",
    "\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "decoder_labels = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_labels\")\n",
    "question_mask = tf.sequence_mask(decoder_lengths, dtype=tf.float32)\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "\n",
    "loss = tf.add(answer_loss, question_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train both parts of the model at once, we add both losses (from the questions and answers) together, and tell our optimizer to minimize the sum.  Neural network training proceeds by iterating over the data in batches, feeding each one to the network and letting the optimizer tune the weights by some variant of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qgen.data import training_data\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    for batch in training_data():\n",
    "        _, loss_value = session.run([optimizer, loss], {\n",
    "            document_tokens: batch[\"document_tokens\"],\n",
    "            document_lengths: batch[\"document_lengths\"],\n",
    "            answer_labels: batch[\"answer_labels\"],\n",
    "            encoder_input_mask: batch[\"answer_masks\"],\n",
    "            encoder_lengths: batch[\"answer_lengths\"],\n",
    "            decoder_inputs: batch[\"question_input_tokens\"],\n",
    "            decoder_labels: batch[\"question_output_tokens\"],\n",
    "            decoder_lengths: batch[\"question_lengths\"],\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(loss_value))\n",
    "    saver.save(session, \"model\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's use it to generate some new questions and answers!  First, we predict some answers.  Notice how we're only feeding the document itself to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from qgen.data import test_data, collapse_documents\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "session = tf.InteractiveSession()\n",
    "saver.restore(session, \"model-5\")\n",
    "\n",
    "batch = next(test_data())\n",
    "batch = collapse_documents(batch)\n",
    "\n",
    "answers = session.run(answer_tags, {\n",
    "    document_tokens: batch[\"document_tokens\"],\n",
    "    document_lengths: batch[\"document_lengths\"],\n",
    "})\n",
    "answers = np.argmax(answers, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some answers, we can use the sequence-to-sequence model to generate some questions that (hopefully) have the predicted answers.  To make new predictions with the decoder, we'll have to change its implementation slightly, to wire up its inputs from its previous outputs instead of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from qgen.data import expand_answers\n",
    "from qgen.embedding import look_up_token, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN\n",
    "\n",
    "batch = expand_answers(batch, answers)\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch[\"size\"]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=16)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    document_tokens: batch[\"document_tokens\"],\n",
    "    document_lengths: batch[\"document_lengths\"],\n",
    "    answer_labels: batch[\"answer_labels\"],\n",
    "    encoder_input_mask: batch[\"answer_masks\"],\n",
    "    encoder_lengths: batch[\"answer_lengths\"],\n",
    "})\n",
    "questions[:,:,UNKNOWN_TOKEN] = 0\n",
    "questions = np.argmax(questions, 2)\n",
    "\n",
    "for i in range(batch[\"size\"]):\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, questions[i])\n",
    "    print(\"Question: \" + \" \".join(look_up_token(token) for token in question))\n",
    "    print(\"Answer: \" + batch[\"answer_text\"][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"resources\"></a>\n",
    "# References, Resources and More\n",
    "\n",
    "- Chris Olah's posts on things like [word representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/), [LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), and [augmented RNNs](https://distill.pub/2016/augmented-rnns/)\n",
    "- [Andrej Karpathy's blog post on character level RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [GRUs vs LSTMs](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n",
    "- [Dev Nag's blog post about pointer networks](https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264)\n",
    "- [Stanford's CS224n: Deep Learning for NLP course notes](http://web.stanford.edu/class/cs224n/syllabus.html)\n",
    "- Sequence2sequence [paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "- Attention for translation [paper](https://arxiv.org/abs/1409.0473)\n",
    "- Pointer networks [paper](https://arxiv.org/abs/1506.03134)\n",
    "- [Maluuba's research blog](http://www.maluuba.com/publications/)\n",
    "- [Actual question generation demo](https://techcrunch.com/2017/05/10/microsoft-maluuba-is-teaching-machines-to-ask-questions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
